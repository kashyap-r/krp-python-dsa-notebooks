{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-22T09:32:09.563583Z",
     "start_time": "2026-01-22T09:32:09.557783Z"
    }
   },
   "source": [
    "# Find all pairs of number in an array that sum to a given target (optimize for large datasets)\n",
    "\n",
    "def find_pairs(nums, target):\n",
    "    seen = {}\n",
    "    pairs = []\n",
    "\n",
    "    for num in nums:\n",
    "        complement = target - num\n",
    "        if complement in seen:\n",
    "            # Add all occurrences of complement seen so far\n",
    "            for _ in range(seen[complement]):\n",
    "                pairs.append((complement, num))\n",
    "        # Track how many times num has appeared\n",
    "        seen[num] = seen.get(num, 0) + 1\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "# Example usage\n",
    "arr = [2, 4, 3, 7, 1, 5, 8, 9, 6]\n",
    "target = 10\n",
    "print(find_pairs(arr, target))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 7), (2, 8), (1, 9), (4, 6)]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# You receive a daily CSV dump from a payment gateway\n",
    "# Question : write python code to 1) ignore invalid amounts, calculate net amount per user, then return a dictionary {user-id: net_amount}\n",
    "\"\"\"\n",
    "1. define the \"invalid amount as\n",
    "2. calculate net amount per user - is it daily net / weekly net\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "def compute_net_amount(file_path):\n",
    "    result = defaultdict(float)\n",
    "    with open(file_path, 'r') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            try:\n",
    "                amount = float(row[\"amount\"])\n",
    "                user_id = row[\"user_id\"]\n",
    "                result[user_id] += amount\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    return dict(result)\n",
    "\n",
    "\"\"\"\n",
    "Follow-ups\n",
    "How does this scale to 10GB files\n",
    "Would you use pandas ?\n",
    "How would you unit test this ?\n",
    "How would you log bad records ?\n",
    "\"\"\"\n"
   ],
   "id": "d2d00fdaffb2ab90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Question:\n",
    "You have two massive tables\n",
    "    - Payments\n",
    "    - refunds\n",
    "\n",
    "How would you endure net amounts never go negative incorrectly due to out-of-order ingestion >\n",
    "\n"
   ],
   "id": "fd5df39306806c4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "Problem: Streaming logic\n",
    "you are given a huge file, find top 5 users by total amount\n",
    "\n",
    "Assumptions:\n",
    "file has only two fields user, amount and is comma spearated\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "\n",
    "def top_users(file_path, k=5):\n",
    "    totals = defaultdict(float)\n",
    "    with open(file_path, 'r') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            user, amount = row.strip().split(\",\")\n",
    "            totals[user] += float(amount)\n",
    "    return heapq.nlargest(k, totals.items(), key=lambda x: x[1])\n",
    "\n",
    "\n",
    "\n",
    "# -- alternate logic (streaming logic ... reads the file line by line )\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "\n",
    "def top5_users_by_amount(file_path):\n",
    "    user_totals = defaultdict(float)\n",
    "\n",
    "    # Stream through the file line by line\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            # Example format: user_id,amount\n",
    "            parts = line.strip().split(',')\n",
    "            if len(parts) != 2:\n",
    "                continue  # skip malformed lines\n",
    "            user_id, amount_str = parts\n",
    "            try:\n",
    "                amount = float(amount_str)\n",
    "            except ValueError:\n",
    "                continue  # skip bad data\n",
    "            user_totals[user_id] += amount\n",
    "\n",
    "    # Use heap to get top 5 efficiently\n",
    "    top5 = heapq.nlargest(5, user_totals.items(), key=lambda x: x[1])\n",
    "    return top5\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    result = top5_users_by_amount(\"huge_file.csv\")\n",
    "    for user, total in result:\n",
    "        print(f\"User {user}: {total:.2f}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Scaling Considerations\n",
    "• \tIf the file is truly massive (billions of rows):\n",
    "• \tConsider chunked processing with .\n",
    "• \tOr use external sort/aggregation tools (Spark, Dask, Flink).\n",
    "• \tIf user IDs are extremely numerous:\n",
    "• \tYou may need a streaming top‑k algorithm (like a min‑heap of size 5 maintained during iteration).\n",
    "• \tThat way you don’t store all users, only the top candidates.\n",
    "\n",
    "\"\"\""
   ],
   "id": "110a8ebeccc78ac4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "Puzzle: Given transaction events\n",
    "events = [\n",
    "('txn1', 100),\n",
    "('txn2', 200),\n",
    "('txn3', 300),\n",
    "('txn1', 100)   # duplicate\n",
    "]\n",
    "\n",
    "Task: Ensure each transaction is processed only once\n",
    "\"\"\"\n",
    "\n",
    "events = [\n",
    "('txn1', 100),\n",
    "('txn2', 200),\n",
    "('txn3', 300),\n",
    "('txn1', 100)   # duplicate\n",
    "]\n",
    "\n",
    "def process_events(events):\n",
    "    seen = ()\n",
    "    total = 0\n",
    "\n",
    "    for txn_id, amount in events:\n",
    "        if txn_id not in seen:\n",
    "            seen.add(txn_id)\n",
    "            total += amount\n",
    "    return total\n",
    "\n",
    "\n",
    "# ---- alternatively\n",
    "\n",
    "processed = set()   # keep track of seen transaction IDs\n",
    "total = 0\n",
    "\n",
    "for txn_id, amount in events:\n",
    "    if txn_id not in processed:\n",
    "        # process only once\n",
    "        total += amount\n",
    "        processed.add(txn_id)\n",
    "\n",
    "print(\"Total processed amount:\", total)\n",
    "print(\"Processed transactions:\", processed)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Follow up questions\n",
    "\n",
    "What if the process crashes ?\n",
    "    Executed stand alone ... data is gone\n",
    "\n",
    "Where would state live ?\n",
    "    - Small scale / Python script: store processed IDs in a database table or Redis set.\n",
    "    - Large scale / production: use a streaming framework with checkpointing (Flink, Spark) or rely on idempotent sinks.\n",
    "    - Critical systems: combine both — checkpoint state and enforce idempotency downstream.\n",
    "\n",
    "\"\"\"\n"
   ],
   "id": "8cfb563ac6fcd1ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Data Quality\n",
    "\"\"\"\n",
    "data = [\"100\", \"abc\", \"\", None, \"200\"]\n",
    "Return sum of valid numbers only\n",
    "\"\"\"\n",
    "\n",
    "data = [\"100\", \"abc\", \"\", None, \"200\"]\n",
    "total = 0\n",
    "for val in data:\n",
    "    try:\n",
    "        total += float(val)\n",
    "    except (TypeError, ValueError):\n",
    "        continue\n",
    "\n",
    "print (total)"
   ],
   "id": "88a29ffc2ef19093"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# One messy pandas dataset problem\n",
    "\n"
   ],
   "id": "ec146dde867da072"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Latest profile per user\n",
   "id": "44f0d158d18acfcb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T14:18:23.839189Z",
     "start_time": "2026-01-22T14:18:23.711163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "transactions = [\n",
    "    {\"user_id\": \"U1\", \"txn_date\": \"2025-01-01\", \"net_amount\": 100},\n",
    "    {\"user_id\": \"U1\", \"txn_date\": \"2025-01-02\", \"net_amount\": 300},\n",
    "    {\"user_id\": \"U1\", \"txn_date\": \"2025-01-03\", \"net_amount\": -50},\n",
    "    {\"user_id\": \"U1\", \"txn_date\": \"2025-01-05\", \"net_amount\": 200},\n",
    "    {\"user_id\": \"U1\", \"txn_date\": \"2025-01-06\", \"net_amount\": 150},\n",
    "\n",
    "    {\"user_id\": \"U2\", \"txn_date\": \"2025-01-01\", \"net_amount\": 500},\n",
    "    {\"user_id\": \"U2\", \"txn_date\": \"2025-01-03\", \"net_amount\": 400},\n",
    "    {\"user_id\": \"U2\", \"txn_date\": \"2025-01-04\", \"net_amount\": -100},\n",
    "    {\"user_id\": \"U2\", \"txn_date\": \"2025-01-08\", \"net_amount\": 250},\n",
    "]\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def rolling_avg_simple(data, window=7):\n",
    "    per_user = defaultdict(list)\n",
    "\n",
    "    # Group by user\n",
    "    for row in data:\n",
    "        per_user[row[\"user_id\"]].append(\n",
    "            (row[\"txn_date\"], row[\"net_amount\"])\n",
    "        )\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Compute rolling averages\n",
    "    for user, records in per_user.items():\n",
    "        records.sort()  # sort by date\n",
    "\n",
    "        for i in range(len(records)):\n",
    "            window_vals = [\n",
    "                amt for _, amt in records[max(0, i-window+1):i+1]\n",
    "            ]\n",
    "            avg = sum(window_vals) / len(window_vals)\n",
    "\n",
    "            results.append({\n",
    "                \"user_id\": user,\n",
    "                \"txn_date\": records[i][0],\n",
    "                \"rolling_avg\": round(avg, 2)\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "output = rolling_avg_simple(transactions)\n",
    "for row in output:\n",
    "    print(row)\n"
   ],
   "id": "d3ffd200a542ee2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_id': 'U1', 'txn_date': '2025-01-01', 'rolling_avg': 100.0}\n",
      "{'user_id': 'U1', 'txn_date': '2025-01-02', 'rolling_avg': 200.0}\n",
      "{'user_id': 'U1', 'txn_date': '2025-01-03', 'rolling_avg': 116.67}\n",
      "{'user_id': 'U1', 'txn_date': '2025-01-05', 'rolling_avg': 137.5}\n",
      "{'user_id': 'U1', 'txn_date': '2025-01-06', 'rolling_avg': 140.0}\n",
      "{'user_id': 'U2', 'txn_date': '2025-01-01', 'rolling_avg': 500.0}\n",
      "{'user_id': 'U2', 'txn_date': '2025-01-03', 'rolling_avg': 450.0}\n",
      "{'user_id': 'U2', 'txn_date': '2025-01-04', 'rolling_avg': 266.67}\n",
      "{'user_id': 'U2', 'txn_date': '2025-01-08', 'rolling_avg': 262.5}\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
